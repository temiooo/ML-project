{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J07-lDzQlMhC"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import glob\n",
        "import os\n",
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "import random"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hJEs1U4DDRD3"
      },
      "source": [
        "Upload the `images.zip` file to colab and unzip using the cells below. Remember that the file upload is ephemeral, so the files are uploaded only for each session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwr0gj29mUhF"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkHVbNPDiRw5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIDI5q-_Drsk"
      },
      "outputs": [],
      "source": [
        "# !unzip images.zip\n",
        "!unzip /content/drive/MyDrive/images.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2w1MqIBQnOkS"
      },
      "source": [
        "The **image** folder contains 5 sub-directories each of which contains images from one of the following image classes: airplanes, cars, dog, faces and keyboard. In each class there are 80 images, the first 60 will be used for training and the rest will be used for testing. \n",
        "\n",
        "An example of the images can be seen below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz9DTaPsncKW"
      },
      "outputs": [],
      "source": [
        "## example of 5 images from each class\n",
        "example_img = list()\n",
        "for root, dirs, _ in os.walk('image/'):\n",
        "    for class_folder in dirs:\n",
        "      img_files = os.listdir(os.path.join(root, class_folder))\n",
        "      img_files = [\n",
        "          os.path.join(*[root, class_folder, filename]) for filename in img_files\n",
        "          ]\n",
        "      example_img.extend(img_files[:5])\n",
        "\n",
        "img_lst = list()\n",
        "for imgf in example_img:\n",
        "    img = cv2.imread(imgf)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_lst.append(img)\n",
        "\n",
        "f, axarr = plt.subplots(5,5,figsize=(15, 15))\n",
        "axarr = axarr.flatten()\n",
        "for img, ax in zip(img_lst, axarr):\n",
        "    ax.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AqOA4UmnSGrD"
      },
      "source": [
        "SIFT (Scale Invariant Fourier Transform) Detector is used in the detection of interest points on an input image. It allows identification of localized features in images. The following script generates images with keypoints for the above examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peV_wb5QTEcd"
      },
      "outputs": [],
      "source": [
        "f, axarr = plt.subplots(5,5,figsize=(15, 15))\n",
        "axarr = axarr.flatten()\n",
        "for img, ax in zip(img_lst, axarr):\n",
        "    # Converting image to grayscale\n",
        "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    # Applying SIFT detector\n",
        "    sift = cv2.xfeatures2d.SIFT_create()\n",
        "    kp = sift.detect(gray, None)\n",
        "    # Marking the keypoint on the image using circles\n",
        "    img=cv2.drawKeypoints(gray,kp,img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "    ax.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xaLYWLTVniU7"
      },
      "source": [
        "The first step is to compute the SIFT descriptors for all images in the data directory. We have precomputed the SIFT image descriptors for 900 predifined patches in each image. These can be imported from `all_features.mat` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TVNv41pVigD"
      },
      "outputs": [],
      "source": [
        "label = {'airplanes':0, 'cars':1, 'dog':2, 'faces':3, 'keyboard':4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGxOYskddOPO"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "mat = loadmat('all_features.mat')\n",
        "train_des = mat['TrainMat']\n",
        "test_des = mat['TestMat']\n",
        "train_label = list()\n",
        "test_label = list()\n",
        "for i in range(5):\n",
        "    train_label.extend([i] * 60)\n",
        "    test_label.extend([i] * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeUZiqRqWHf7"
      },
      "outputs": [],
      "source": [
        "train_des = train_des.reshape((300, -1, 128))\n",
        "test_des = test_des.reshape((100, -1, 128))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2y3Rnvlxn79D"
      },
      "source": [
        "## 2. Dictionary Creation - Feature Quantization\n",
        "\n",
        "Task 1: Using [sklearn KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), create a dictionary by clustering a **subset** (eg. 6000) of the extracted descriptors. \n",
        "\n",
        "Use a dictionary of 500 words and the `elkan` algorithm to compute the dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odE43mONooNG"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# Step 1: create an array (N x 128) of all descriptors in the training set (not by image)\n",
        "train_des_2 = train_des.reshape((-1, 128))\n",
        "\n",
        "# Step 2: use Kmeans to create the Dictionary (the resulting Dictionary should have K words and 128 features)\n",
        "random_indices = random.sample(range(train_des_2.shape[0]), k=60000)\n",
        "subset = train_des_2[(random_indices)]\n",
        "\n",
        "# To create dictionary from subset\n",
        "kmeans = KMeans(n_clusters=500, algorithm='elkan').fit(subset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "stJoQfvBpLjJ"
      },
      "source": [
        "Task 2: Assign each image descriptor in the training and test sets, to the nearest codeword cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfXhKrMApl9f"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "test_des_2 = test_des.reshape((-1, 128))\n",
        "\n",
        "train_cluster = kmeans.predict(train_des_2)\n",
        "test_cluster = kmeans.predict(test_des_2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E4HDos5XprFo"
      },
      "source": [
        "## 3. Image Representation using BoW\n",
        "Task 3: Represent each image in the training and the test dataset as a histogram of visual words (i.e. represent each image using the Bag of Words representation). Normalise the histograms by their L1 norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq8tI6-LqNzH"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# Step 1: For each image, create a histogram of the descriptors\n",
        "# i.e. a histogram of the allocated clusters\n",
        "train_hist = list()\n",
        "test_hist = list()\n",
        "\n",
        "train_cluster = train_cluster.reshape((300, -1))\n",
        "test_cluster = test_cluster.reshape((100, -1))\n",
        "\n",
        "for i in range(300):\n",
        "  hist = np.zeros(500, dtype=\"int\")\n",
        "  for val in train_cluster[i]:\n",
        "    hist[val] = hist[val] + 1\n",
        "  train_hist.append(hist)\n",
        "\n",
        "for j in range(100):\n",
        "  hist = np.zeros(500, dtype=\"int\")\n",
        "  for val in test_cluster[j]:\n",
        "    hist[val] = hist[val] + 1\n",
        "  test_hist.append(hist)\n",
        "\n",
        "train_hist = np.array(train_hist)\n",
        "test_hist = np.array(test_hist)\n",
        "\n",
        "\n",
        "# Step 2: Normalize by the L1 norm of the vector\n",
        "l1_norm = train_hist[0].sum()\n",
        "train_hist = train_hist/l1_norm\n",
        "test_hist = test_hist/l1_norm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eoCiLwDWqWwS"
      },
      "source": [
        "## 4. Image Classification using a Nearest Neighbour Classifier\n",
        "Task 4: Implement the Euclidean distance for the multi-dimensional case. Using sklearn [KNN classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and the distance implemented  (passed using keyword argument `metric`, check KNN API for details), train a model on the train set BoW, for K=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w5Vf5_SrocT"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(hist_1: np.array, hist_2: np.array):\n",
        "    \"\"\"\n",
        "      hist_1: a 1D vector representing a histogram\n",
        "      hist_2: a 1D vector representing a histogram\n",
        "      returns:\n",
        "      The distance between two histograms (float)\n",
        "    \"\"\"\n",
        "    dist = 0\n",
        "    ### YOUR CODE HERE\n",
        "    diff = ((hist_2 - hist_1) ** 2)\n",
        "    sum = diff.sum()\n",
        "    dist = np.sqrt(sum)\n",
        "    return dist\n",
        "\n",
        "### YOUR CODE HERE\n",
        "euclidean_classifier = KNeighborsClassifier(n_neighbors=1, metric=euclidean_distance)\n",
        "euclidean_classifier.fit(train_hist, np.array(train_label))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XjbkaJW4rqxR"
      },
      "source": [
        "Task 5: Implement a method for histogram intersection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nALLNqJNrxGH"
      },
      "outputs": [],
      "source": [
        "def hist_intersection(hist_1: np.array, hist_2: np.array):\n",
        "  \"\"\"\n",
        "    hist_1: a 1D vector representing a histogram\n",
        "    hist_2: a 1D vector representing a histogram\n",
        "    returns:\n",
        "    The distance between two histograms (float)\n",
        "  \"\"\"\n",
        "  dist = 0.\n",
        "  ### YOUR CODE HERE\n",
        "  dist = np.minimum(hist_1, hist_2).sum()\n",
        "  return 1 - dist"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm47V_IHr3Gw"
      },
      "source": [
        "Task 6: Train a second classifier using the `hist_intersection()` as the distance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6LMqHAKr2mQ"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "hist_intersection_classifier = KNeighborsClassifier(n_neighbors=1, metric=hist_intersection)\n",
        "hist_intersection_classifier.fit(train_hist, np.array(train_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQZtz1mi08xu"
      },
      "outputs": [],
      "source": [
        "# Evaluating the euclidean classifier on the test set\n",
        "euclidean_pred = euclidean_classifier.predict(test_hist)\n",
        "\n",
        "euclidean_score = accuracy_score(test_label, euclidean_pred, normalize=False)\n",
        "print(\"Accuracy for euclidean classifier is {}%\".format(euclidean_score))\n",
        "\n",
        "euclidean_cm = confusion_matrix(test_label, euclidean_pred)\n",
        "euclidean_cm_display = ConfusionMatrixDisplay(euclidean_cm, display_labels=list(label)).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMeDXdqQ5Apb"
      },
      "outputs": [],
      "source": [
        "# Evaluating the histogram intersection based classifier on the test set\n",
        "hist_intersection_pred = hist_intersection_classifier.predict(test_hist)\n",
        "\n",
        "hist_intersection_score = accuracy_score(test_label, hist_intersection_pred, normalize=False)\n",
        "print(\"Accuracy for hist_intersection classifier is {}%\".format(hist_intersection_score))\n",
        "\n",
        "hist_intersection_cm = confusion_matrix(test_label, hist_intersection_pred)\n",
        "hist_intersection_cm_display = ConfusionMatrixDisplay(hist_intersection_cm, display_labels=list(label)).plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pjv5l0Ws4lp"
      },
      "source": [
        "## 5. Dictionary size\n",
        "\n",
        "Task 7: Repeat steps 1-6 using a very small dictionary size (eg. 5). Compute the accuracy and confusion matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQjQn1wptdGH"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# Step 1\n",
        "kmeans = KMeans(n_clusters=5, algorithm='elkan').fit(subset)\n",
        "\n",
        "# Step 2\n",
        "train_cluster2 = kmeans.predict(train_des_2)\n",
        "test_cluster2 = kmeans.predict(test_des_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "465cs9c86h0V"
      },
      "outputs": [],
      "source": [
        "# Step 3\n",
        "train_hist2 = list()\n",
        "test_hist2 = list()\n",
        "\n",
        "train_cluster2 = train_cluster2.reshape((300, -1))\n",
        "test_cluster2 = test_cluster2.reshape((100, -1))\n",
        "\n",
        "for i in range(300):\n",
        "  hist = np.zeros(500, dtype=\"int\")\n",
        "  for val in train_cluster2[i]:\n",
        "    hist[val] = hist[val] + 1\n",
        "  train_hist2.append(hist)\n",
        "\n",
        "for j in range(100):\n",
        "  hist = np.zeros(500, dtype=\"int\")\n",
        "  for val in test_cluster2[j]:\n",
        "    hist[val] = hist[val] + 1\n",
        "  test_hist2.append(hist)\n",
        "\n",
        "train_hist2 = np.array(train_hist2)\n",
        "test_hist2 = np.array(test_hist2)\n",
        "\n",
        "\n",
        "# Normalize by the L1 norm of the vector\n",
        "l1_norm = train_hist2[0].sum()\n",
        "train_hist2 = train_hist2/l1_norm\n",
        "test_hist2 = test_hist2/l1_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgNeUXi66_J0"
      },
      "outputs": [],
      "source": [
        "# Step 4\n",
        "euclidean_classifier2 = KNeighborsClassifier(n_neighbors=1, metric=euclidean_distance)\n",
        "euclidean_classifier2.fit(train_hist2, np.array(train_label))\n",
        "\n",
        "# Step 5\n",
        "hist_intersection_classifier2 = KNeighborsClassifier(n_neighbors=1, metric=hist_intersection)\n",
        "hist_intersection_classifier2.fit(train_hist2, np.array(train_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkFEmuQz7Q6z"
      },
      "outputs": [],
      "source": [
        "# Step 6 - euclidean classifier\n",
        "# Evaluating the euclidean classifier on the test set\n",
        "euclidean_pred2 = euclidean_classifier2.predict(test_hist2)\n",
        "\n",
        "euclidean_score2 = accuracy_score(test_label, euclidean_pred2, normalize=False)\n",
        "print(\"Accuracy for euclidean classifier when k=5 is {}%\".format(euclidean_score2))\n",
        "\n",
        "euclidean_cm2 = confusion_matrix(test_label, euclidean_pred2)\n",
        "euclidean_cm_display2 = ConfusionMatrixDisplay(euclidean_cm2, display_labels=list(label)).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QouVN3S7Wiw"
      },
      "outputs": [],
      "source": [
        "# Step 6 - histogram intersection classifier\n",
        "hist_intersection_pred2 = hist_intersection_classifier2.predict(test_hist2)\n",
        "\n",
        "hist_intersection_score2 = accuracy_score(test_label, hist_intersection_pred2, normalize=False)\n",
        "print(\"Accuracy for hist_intersection classifier when k=5 is {}%\".format(hist_intersection_score2))\n",
        "\n",
        "hist_intersection_cm2 = confusion_matrix(test_label, hist_intersection_pred2)\n",
        "hist_intersection_cm_display2 = ConfusionMatrixDisplay(hist_intersection_cm2, display_labels=list(label)).plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MATPN_ectetj"
      },
      "source": [
        "## 6. Support Vector Machines\n",
        "In this section we will train a linear [SVM classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
        "\n",
        "\n",
        "Task 8: Using [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) select optimal hyperparameters `C` and `gamma`. \n",
        "\n",
        "Evaluate SVC classifier on the test set (in terms of accuracy and confusion matrices) and compare to the KNN classifier results.\n",
        "For each class show some images that are correctly classified and some images that are incorrectly classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw-Zs5luv2F9"
      },
      "outputs": [],
      "source": [
        "parameters = {'gamma':[2**x for x in np.arange(-1, 1.6, 0.1)], 'C':[2**x for x in range(1, 11)]}\n",
        "\n",
        "### YOUR CODE HERE\n",
        "svclassifier = svm.SVC()\n",
        "clf = GridSearchCV(svclassifier, parameters)\n",
        "clf.fit(train_hist, train_label)\n",
        "\n",
        "svc_pred = clf.predict(test_hist)\n",
        "svc_score = accuracy_score(test_label, svc_pred, normalize=False)\n",
        "print(\"Accuracy for SVM classifier is {}%\".format(svc_score))\n",
        "\n",
        "svc_cm = confusion_matrix(test_label, svc_pred)\n",
        "svc_cm_display = ConfusionMatrixDisplay(svc_cm, display_labels=list(label)).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loO3CnwMnj4H"
      },
      "outputs": [],
      "source": [
        "classifications = np.where(test_label == svc_pred, True, False)\n",
        "classifications = classifications.reshape((5, -1))\n",
        "\n",
        "# Get the index of the good and bad predictions for all the classes\n",
        "# Adding 61 to the index values to offset the first 60 training images and 1 because the images are labelled starting with 1 and not 0.\n",
        "good_airplanes = np.where(classifications[0])[0] + 61\n",
        "bad_airplanes = np.where(classifications[0] == False)[0] + 61\n",
        "\n",
        "good_cars = np.where(classifications[1])[0] + 61\n",
        "bad_cars = np.where(classifications[1] == False)[0] + 61\n",
        "\n",
        "good_dogs = np.where(classifications[2])[0] + 61\n",
        "bad_dogs = np.where(classifications[2] == False)[0] + 61\n",
        "\n",
        "good_faces = np.where(classifications[3])[0] + 61\n",
        "bad_faces = np.where(classifications[3] == False)[0] + 61\n",
        "\n",
        "good_keyboards = np.where(classifications[4])[0] + 61\n",
        "bad_keyboards = np.where(classifications[4] == False)[0] + 61"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klB6Dzui9tsT"
      },
      "outputs": [],
      "source": [
        "print(\"Airplanes - correct predictions - image numbers\", good_airplanes)\n",
        "print(\"Airplanes - wrong predictions - image numbers\", bad_airplanes)\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Cars - correct predictions - image numbers\", good_cars)\n",
        "print(\"Cars - wrong predictions - image numbers\", bad_cars)\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Dog - correct predictions - image numbers\", good_dogs)\n",
        "print(\"Dog - wrong predictions - image numbers\", bad_dogs)\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Faces - correct predictions - image numbers\", good_faces)\n",
        "print(\"Faces - bad predictions - image numbers\", bad_faces)\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Keyboard - correct predictions for images\", good_keyboards)\n",
        "print(\"Keyboards - bad predictions - image numbers\", bad_keyboards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4qmeKHofIbO"
      },
      "outputs": [],
      "source": [
        "# Get the id of the images that were wrongly classified and the class they were wrongly classified into\n",
        "gt_pred = np.stack((np.array(test_label), svc_pred), axis=1)\n",
        "gt_pred = gt_pred.reshape((5, -1, 2))\n",
        "\n",
        "result = []\n",
        "for class_pred in gt_pred:\n",
        "  class_result = []\n",
        "  for j, val in enumerate(class_pred):\n",
        "\n",
        "    if (val[0] != val[1]):\n",
        "      class_result.append([j + 61, val[1]])\n",
        "  result.append(class_result)\n",
        "\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
